{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "In this notebook we will implement some dataset analysis techniques to integrate into Suggester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modules\n",
    "import importlib\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import modules.models.Linear as Linear\n",
    "from modules.ActiveLearning import Samplings\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from modules import ActiveLearning\n",
    "from modules import Suggester\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "importlib.reload(modules)\n",
    "importlib.reload(modules.models.Linear)\n",
    "importlib.reload(modules.ActiveLearning)\n",
    "importlib.reload(modules.Suggester)\n",
    "\n",
    "\n",
    "random_seed = 42\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfusion:\n",
    "    def __init__(self, top_n=10):\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def get_most_frequently_confused_classes(self, model, X_train, y_train):\n",
    "        y_pred = model.predict(X_train)\n",
    "        conf_dict = {}\n",
    "        for real, pred in zip(y_train, y_pred):\n",
    "            if (real != pred):\n",
    "                mask = (real,pred)\n",
    "                conf_dict[mask] = conf_dict.get(mask, 0) + 1\n",
    "        lst = list(conf_dict.items())\n",
    "        lst = sorted(lst, key=lambda x: -1 * (conf_dict.get((x[0][1],x[0][0]),0) + x[1]))\n",
    "        tmp = {}\n",
    "        successfully_added = 0\n",
    "        for key, val in lst:\n",
    "            if successfully_added >= self.top_n:\n",
    "                break\n",
    "            if (tmp.get((key[1],key[0]),-1) == -1):\n",
    "                tmp[key] = (val, conf_dict.get((key[1], key[0]),0))\n",
    "                successfully_added += 1\n",
    "        return tmp\n",
    "    \n",
    "    def pretty_print(self, model, X_train, y_train):\n",
    "        tmp = self.get_most_frequently_confused_classes(model, X_train, y_train)\n",
    "        for key, value in tmp.items():\n",
    "            print(f\"{key[0]} was mistaken for {key[1]} {value[0]} times and {key[1]} for {key[0]} {value[1]} times\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindZeroSamples:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-7\n",
    "    \n",
    "    def get_zero_samples_count(self, X):\n",
    "        print(np.argwhere(np.all(abs(X) < self.epsilon, axis=1)))\n",
    "        return np.count_nonzero(np.all(abs(X) < self.epsilon, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset():\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    vectorized_output_path = \"selfpost/vectorized.npy\"\n",
    "    vectorized_labels_output_path = \"selfpost/vectorized_labels.npy\"\n",
    "    with open(vectorized_output_path, \"rb\") as vect_X, open(vectorized_labels_output_path, \"rb\") as vect_y:\n",
    "        X = np.load(vect_X, allow_pickle=True)\n",
    "        y = np.load(vect_y, allow_pickle=True)\n",
    "    le.fit(y)\n",
    "    y = le.transform(y)\n",
    "    return X, y, le.get_params() \n",
    "\n",
    "X, y, y_dict = import_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 50009.59it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "random.seed(random_seed)\n",
    "sug = Suggester.Suggester(X,y, test_fraction=0.99)\n",
    "model = Linear.LinearModelTorch(Linear.LogReg(), 100)\n",
    "model.train(sug.X_train, sug.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconf = TrainConfusion(20)\n",
    "tconf.pretty_print(model, sug.X_train, sug.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispute measure: how much n_nearest nearest points' known class counts are unbalanced\n",
    "# Не уверен, как это впихнуть в саджестр, потому что не совпадает интерфейс с остальными\n",
    "# методами активлернинга. Возможно, я передаю интерфейс get_samples_for_labeling и буду \n",
    "# передавать туда сам suggester\n",
    "class LabelDisputablePoints:\n",
    "    def __init__(self, n_top=1000, n_nearest=100):\n",
    "        self.n_top = n_top\n",
    "        self.n_nearest = n_nearest\n",
    "        \n",
    "# Придумать более адекватную реализацию)\n",
    "    def get_samples_for_labeling(self, sug, X_test, y_test):\n",
    "        print(X_test.shape, y_test.shape)\n",
    "        dist, ind = sug.index.search(sug.X_test, self.n_nearest)\n",
    "        entropies = np.zeros(sug.X.shape[0], np.float32)\n",
    "        for i, idx in tqdm(enumerate(ind)):\n",
    "            mask = np.zeros(sug.X.shape[0], np.bool)\n",
    "            mask[idx] = True\n",
    "            mask = mask * sug.is_train_mask\n",
    "            neigh = np.bincount(sug.y[mask])\n",
    "            entropies[i] = entropy(neigh, axis=0)\n",
    "        indices_to_return = np.argsort(entropies)[::-1]\n",
    "        return indices_to_return[:self.n_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from modAL.models.base import BaseEstimator\n",
    "from modAL.utils.data import modALinput\n",
    "from functools import partial\n",
    "from modAL.utils.selection import multi_argmax, shuffled_argmax\n",
    "\n",
    "def _nearest_neighbours_to_entropy(nearest_neighbours: np.ndarray, min_bins: int):\n",
    "    bin_count = np.apply_along_axis(partial(np.bincount, minlength=min_bins), 1, nearest_neighbours)\n",
    "    return entropy(bin_count, axis=1)\n",
    "\n",
    "\n",
    "def disputable_points(classifier: BaseEstimator, X: modALinput,\n",
    "                               index: IndexFlatL2, n_nearest: int = 100,\n",
    "                               n_instances: int = 1, random_tie_break: bool = False,\n",
    "                               **uncertainty_measure_kwargs):\n",
    "    dist, ind = index.search(X, n_nearest)\n",
    "    entropies = _nearest_neighbours_to_entropy(classifier.y_training[ind], np.unique(classifier.y_training).shape[0])\n",
    "    indices_to_return = np.argsort(entropies)[::-1]\n",
    "    return indices_to_return[:n_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_elements = 100000\n",
    "subsample = np.random.choice(X.shape[0], num_of_elements, replace=False)\n",
    "sug = Suggester.Suggester(X[subsample],y[subsample], test_fraction=0.8, build_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp = LabelDisputablePoints()\n",
    "ldp.get_samples_for_labeling(sug, sug.X_test, sug.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.09861229, 0.63651417])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1, 1, 1], [1, 2, 3], [1, 2, 2]])\n",
    "_nearest_neighbours_to_entropy(arr, min_bins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import entropy_sampling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modules.models import Linear\n",
    "importlib.reload(Linear)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, train_size=0.01, random_state=random_seed)\n",
    "index = faiss.IndexFlatL2(X_test.shape[1])\n",
    "index.add(X_train.astype(np.float32))\n",
    "\n",
    "learner = ActiveLearner(\n",
    "    estimator=Linear.LinearModelTorch(Linear.LogReg(), 100),\n",
    "    query_strategy=disputable_points,\n",
    "    X_training=X_train, y_training=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx, query_inst = learner.query(X_test.astype(np.float32), index=index, n_instances=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57438  47534  17361  56281  55553  50680  16802  52674  50242  70260\n",
      "  53060  71813  60726  40455  70972  56816  50172  83366  67631  30613\n",
      "  98789  97989  33731  36447  55315  16865 100338   4367   4992  17184\n",
      "  63143  16705  64433  29503  37309  42989  68123  57645  16925  15182\n",
      "  88186  86248  96934  48148  70632  89089  42561  94993  11871  62771\n",
      "  90671  54461  71559  23168  38862  45270  86572  62658  47845  80249\n",
      "  31005   5433  12292  27428  42143   6954   7029  42073  44592  16587\n",
      "  60016  92934  11210  71787  21243  20364  28385  18567   1148  89075\n",
      "  56296  17103    848  43754  61463   9523  30271  55540  57215  14670\n",
      "  59491  68820  56912   4014  30474  96031  23807  10289  18179  56291]\n"
     ]
    }
   ],
   "source": [
    "print(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 31005  17184  27428  33731  16925  60726   6954  96934  17361  16587\n",
      "  86572  54461  98789  56816  50680  67631  70260  62658   7029  60016\n",
      "  53060  43754  70972  30613  40455  97989  50242  62771  36447  90671\n",
      "  38862  11210  50172  47534  12292  83366  57438  28385  47845  16865\n",
      "  71813  17103  52674  63143  16802  48148  55553  15182  68123  70632\n",
      "   1148  56281  18567  55315  37309  42561  92934  56296 100338  42073\n",
      "  64433  20364  42143   5433   4992  11871  80249  89089  23168  89075\n",
      "  21243  71559  44592    848  42989  23807  86248  45270  94993  57645\n",
      "  88186  71787   4014  55540  30474  29503  16705  30271   4367  61463\n",
      "  96031  57215  56912  59491  14670  10289   9523  68820  18179  39459]\n"
     ]
    }
   ],
   "source": [
    "print(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoLabeling(ActiveLearningBase):\n",
    "    def __init__(self, n_top=1000):\n",
    "        super().__init__(n_top)\n",
    "        self.n_top = n_top\n",
    "\n",
    "    def get_samples_for_labeling(self, model, X_test, y_test):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        max_ind = np.argmax(y_proba, axis=1)\n",
    "        y_proba = np.max(y_proba, axis=1)\n",
    "        ind = np.lexsort((max_ind, y_proba))[::-1]\n",
    "        ind_to_return = ind[:min(self.n_top, y_proba.shape[0])]\n",
    "        return \"labeling\", ind_to_return, max_ind[ind_to_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudolabeling(classifier: BaseEstimator, X: modALinput,\n",
    "                               n_instances: int = 1, random_tie_break: bool = False,\n",
    "                               **uncertainty_measure_kwargs):\n",
    "        y_proba = classifier.predict_proba(X)\n",
    "        entropies = entropy(y_proba, axis=1)\n",
    "        if not random_tie_break:\n",
    "            return multi_argmax(entropies, n_instances=n_instances)\n",
    "\n",
    "        return shuffled_argmax(entropies, n_instances=n_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from modules.ActiveLearning import Heuristics\n",
    "learner = ActiveLearner(\n",
    "    estimator=Linear.LinearModelTorch(Linear.LogReg(), 100),\n",
    "    query_strategy=Heuristics.pseudolabeling,\n",
    "    X_training=X_train, y_training=y_train\n",
    ")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 97153  37280  32696  72395  52367  87998  28018  50899  20218  51106\n",
      "  37962  20245  64953  84674  77207  51165  49718  40779  81104  28722\n",
      "   3784  54333  98781  42057   7698   4815  12169  52281  20343 101145\n",
      "  85234  11335  99270  29636  34824  14096  45884  81811  61271  62900\n",
      "  29762   2270  38713  12525  57284  22845  10327  41140   1194  35384\n",
      "  42388  47340  32524  87336  93778  14033  54854  68060  41224  10133\n",
      "  79071  71149  74153  44389  47224  53922  25879  52013  30363  44246\n",
      "  24659  30393  94253  85820  49124  51261  45219  57632  29903  42954\n",
      "  67739  99845  75729  64090  37802  28820  67671  49308  86421  99017\n",
      "  80174  29243  67466  74939  64502  73943  17153  91719  82439  95477]\n"
     ]
    }
   ],
   "source": [
    "query_idx, query_inst = learner.query(X_test.astype(np.float32), index=index, n_instances=100)\n",
    "print(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.48it/s]\n"
     ]
    }
   ],
   "source": [
    "learner.teach(X_test[query_idx].astype(np.float32), learner.predict(X_test[query_idx].astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_random_sampling(classifier: BaseEstimator, X: modALinput,\n",
    "                               n_instances: int = 1, random_tie_break: bool = False,\n",
    "                               **uncertainty_measure_kwargs):\n",
    "    ind = np.random.choice(X_test.shape[0], self.n_top, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from modAL.models.base import BaseEstimator\n",
    "from modAL.utils.data import modALinput\n",
    "from functools import partial\n",
    "from modAL.utils.selection import multi_argmax, shuffled_argmax\n",
    "\n",
    "def _nearest_neighbours_to_entropy(nearest_neighbours: np.ndarray, min_bins: int):\n",
    "    bin_count = np.apply_along_axis(partial(np.bincount, minlength=min_bins), 1, nearest_neighbours)\n",
    "    return entropy(bin_count, axis=1)\n",
    "\n",
    "\n",
    "def disputable_points(classifier: BaseEstimator, X: modALinput,\n",
    "                               index: IndexFlatL2, n_nearest: int = 100,\n",
    "                               n_instances: int = 1, random_tie_break: bool = False,\n",
    "                               **uncertainty_measure_kwargs):\n",
    "    dist, ind = index.search(X, n_nearest)\n",
    "    entropies = _nearest_neighbours_to_entropy(classifier.y_training[ind], np.unique(classifier.y_training).shape[0])\n",
    "    indices_to_return = np.argsort(entropies)[::-1]\n",
    "    print(entropies[indices_to_return[:n_instances]])\n",
    "    return indices_to_return[:n_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ActiveLearner(\n",
    "    estimator=Linear.LinearModelTorch(Linear.LogReg(), 100),\n",
    "    query_strategy=disputable_points,\n",
    "    X_training=X_train, y_training=y_train\n",
    ")\n",
    "query_idx, query_inst = learner.query(X_test.astype(np.float32), index=index, n_instances=100)\n",
    "print(query_idx)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nearest_neighbours_to_entropy(nearest_neighbours: np.ndarray, min_bins: int):\n",
    "    bin_count = np.apply_along_axis(partial(np.bincount, minlength=min_bins), 1, nearest_neighbours)\n",
    "    return entropy(bin_count, axis=1)\n",
    "\n",
    "\n",
    "def disputable_points(classifier: BaseEstimator, X: modALinput,\n",
    "                               index: IndexFlatL2, n_nearest: int = 100,\n",
    "                               n_instances: int = 1, random_tie_break: bool = False,\n",
    "                               **uncertainty_measure_kwargs):\n",
    "    dist, ind = index.search(X, n_nearest)\n",
    "    entropies = _nearest_neighbours_to_entropy(classifier.y_training[ind], np.unique(classifier.y_training).shape[0])\n",
    "    if not random_tie_break:\n",
    "        print(entropies[multi_argmax(entropies, n_instances=n_instances)])\n",
    "        return multi_argmax(entropies, n_instances=n_instances)\n",
    "\n",
    "    return shuffled_argmax(entropies, n_instances=n_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ActiveLearner(\n",
    "    estimator=Linear.LinearModelTorch(Linear.LogReg(), 100),\n",
    "    query_strategy=disputable_points,\n",
    "    X_training=X_train, y_training=y_train\n",
    ")\n",
    "query_idx, query_inst = learner.query(X_test.astype(np.float32), index=index, n_instances=100)\n",
    "print(query_idx)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
