{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TOPAPEC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TOPAPEC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import faiss\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import modules\n",
    "import pickle\n",
    "import sys\n",
    "import numpy\n",
    "import logging\n",
    "import modules.ActiveLearning as ActiveLearning\n",
    "import modules.preprocess as preprocess\n",
    "from modules.ModelWrap import ModelWrap\n",
    "from modules.ActiveLearningBase import ActiveLearningBase\n",
    "from modules.Suggester import Suggester\n",
    "from modules.Suggest import Suggest\n",
    "importlib.reload(modules)\n",
    "importlib.reload(modules.Suggest)\n",
    "importlib.reload(modules.Suggester)\n",
    "importlib.reload(modules.ActiveLearningBase)\n",
    "importlib.reload(modules.ActiveLearning)\n",
    "importlib.reload(preprocess)\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "from multiprocessing import Pool\n",
    "from sklearn import preprocessing\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=100000)\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPAPEC\\anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# This time we are using reddit self-post classification dataset https://www.kaggle.com/mswarbrickjones/reddit-selfposts/?select=subreddit_info.csv\n",
    "# We will assume that post title and post body can be concatenated into one text column.\n",
    "embeddings_dim = 300\n",
    "tqdm.pandas()\n",
    "\n",
    "def preprocess_and_save_dataset(dataset_path, preprocessed_path):\n",
    "    dataframe = pd.read_csv(dataset_path, sep='\\t')\n",
    "    dataframe.loc[:, \"text\"] = dataframe.loc[:, \"title\"] + \" \" + dataframe.loc[:, \"selftext\"]\n",
    "    dataframe = dataframe.drop([\"title\", \"selftext\"], axis=1)\n",
    "    preprocess_pipeline(dataframe)\n",
    "    dataframe.to_pickle(preprocessed_path)\n",
    "    del dataframe\n",
    "\n",
    "def vectorize_and_save_dataset(pickle_path, output_path, output_path_labels):\n",
    "    dataset = pd.read_pickle(pickle_path)\n",
    "    vectorized = np.zeros((dataset.shape[0], embeddings_dim))\n",
    "    embeddings = get_glove_reddit_embeddings()\n",
    "    for i, (vec, row) in enumerate(zip(vectorized, dataset.iterrows())):\n",
    "        vectorized[i] = preprocess.row_to_embedding(row, embeddings, embeddings_dim)\n",
    "    del embeddings\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        np.save(file, vectorized)\n",
    "    with open(output_path_labels, \"wb\") as file:\n",
    "        np.save(file, dataset.loc[:, \"subreddit\"].to_numpy())\n",
    "    del dataset\n",
    "\n",
    "    \n",
    "def preprocess_pipeline(dataset):\n",
    "    cores = 12\n",
    "    multicore_tok(dataset, cores)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    dataset.loc[:, \"text\"].progress_apply(lemmatize_sent, args=[wnl])\n",
    "\n",
    "def lemmatize_sent(wordlist, wnl):\n",
    "    return ' '.join([wnl.lemmatize(w) for w in wordlist])\n",
    "    \n",
    "def multicore_tok(dataset, cores=6):\n",
    "    with Pool(processes=cores) as pool:\n",
    "        dataset.loc[:, \"text\"] = pool.map(nltk.word_tokenize, dataset.loc[:, \"text\"])\n",
    "\n",
    "def multicore_lem(dataset, cores=6):\n",
    "    with Pool(processes=cores) as pool:\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for i, line in tqdm(enumerate(dataset.text)):\n",
    "            dataset.loc[i,\"text\"] = pool.map(wnl.lemmatize, dataset.loc[i, \"text\"])\n",
    "            \n",
    "def get_glove_reddit_embeddings():\n",
    "    # Number of words - 1623397 \n",
    "    embeddings = {}\n",
    "    tmp = []\n",
    "    with io.open(\"GloVe.Reddit.120B.300D.txt\", \"r\", encoding='utf-8') as file:\n",
    "        file.readline()\n",
    "        for line in tqdm(file, total=1623397):\n",
    "            tmp.append(line)\n",
    "    with Pool(processes=14) as pool:\n",
    "        tmp = list(tqdm(pool.imap(preprocess.fetch_embeddings_value, tmp, chunksize=200000), total=1623397))\n",
    "    for word, vector in tqdm(tmp):\n",
    "        embeddings[word] = vector\n",
    "    del tmp\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"selfpost/rspct.tsv\"\n",
    "preprocessed_path = \"selfpost/preprocessed.pkl\"\n",
    "preprocess_and_save_dataset(dataset_path, preprocessed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_output_path = \"selfpost/vectorized.npy\"\n",
    "vectorized_labels_output_path = \"selfpost/vectorized_labels.npy\"\n",
    "vectorize_and_save_dataset(preprocessed_path, vectorized_output_path, vectorized_labels_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "class LinearModel(ModelWrap):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.last_loss = 0\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        self.model.train(X,y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def get_last_loss(self, X_train, y_train):\n",
    "        return log_loss(y_train, model.predict_proba(X_train), eps=1e-15)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "class ConfidenceSamplingSuggestion(ActiveLearningBase):\n",
    "    def __init__(self, n_top=1000):\n",
    "        self.n_top = n_top\n",
    "\n",
    "    def get_samples_for_labeling(self, model, X_test, y_test):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        y_proba = np.max(y_proba, axis=1)\n",
    "        ind = np.lexsort((y_test, y_proba))\n",
    "        return \"oracle\", ind[:min(self.n_top, y_proba.shape[0])]\n",
    "\n",
    "class MarginSampling(ActiveLearningBase):\n",
    "    def __init__(self, n_top=1000):\n",
    "        self.n_top = n_top\n",
    "        \n",
    "    def get_samples_for_labeling(self, model, X_test, y_test):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        y_proba = np.sort(y_proba, axis=1)[:,::-1]\n",
    "        y_proba = y_proba[:,0] - y_proba[:,1]\n",
    "        ind = np.lexsort((y_test, y_proba))\n",
    "        return \"oracle\", ind[:min(self.n_top, y_proba.shape[0])]\n",
    "\n",
    "class EntropySampling(ActiveLearningBase):\n",
    "    def __init__(self, n_top=1000):\n",
    "        self.n_top = n_top\n",
    "        \n",
    "    def get_samples_for_labeling(self, model, X_test, y_test):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        y_proba = entropy(y_proba, axis=1)\n",
    "        ind = np.lexsort((y_test, y_proba))[::-1]\n",
    "        print(y_proba[ind[0]], y_proba[ind[-1]])\n",
    "        return \"oracle\", ind[:min(self.n_top, y_proba.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "vectorized_output_path = \"selfpost/vectorized.npy\"\n",
    "vectorized_labels_output_path = \"selfpost/vectorized_labels.npy\"\n",
    "with open(vectorized_output_path, \"rb\") as vect_X, open(vectorized_labels_output_path, \"rb\") as vect_y:\n",
    "    X = np.load(vect_X, allow_pickle=True)\n",
    "    y = np.load(vect_y, allow_pickle=True)\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "del le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 22.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['selfpost/models/logreg_100it']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"selfpost/models/logreg_100it\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_seed)\n",
    "model = LogisticRegression(random_state=random_seed, n_jobs=-1, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "dump(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.06228423,  0.06092767, -0.14749891, ..., -0.3225482 ,\n",
       "          0.14105452,  0.00854856],\n",
       "        [ 0.08068533, -0.02982698, -0.12298278, ..., -0.27350412,\n",
       "          0.05903656,  0.0867618 ],\n",
       "        [ 0.03988495,  0.06417522, -0.20442137, ..., -0.29071138,\n",
       "          0.03378086, -0.01860267],\n",
       "        ...,\n",
       "        [ 0.15182757,  0.10873834, -0.17133273, ..., -0.17019229,\n",
       "          0.15758706,  0.04102596],\n",
       "        [ 0.04931326,  0.03915333, -0.15517242, ..., -0.30393526,\n",
       "          0.0845803 , -0.00351562],\n",
       "        [ 0.09860376,  0.01065233, -0.13773885, ..., -0.28388423,\n",
       "          0.1883342 , -0.11135513]]),\n",
       " array([470, 445, 853, ...,   9, 288, 667]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"selfpost/models/logreg_100it\"\n",
    "model = load(model_path)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_seed)\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrappedModel = LinearModel(model)\n",
    "confSuggest = ConfidenceSamplingSuggestion(10000)\n",
    "sampl_ind = rng.choice(X_test.shape[0], 1000, replace=False)\n",
    "ind = confSuggest.get_samples_for_labeling(model, X_test[sampl_ind, :], y_test[sampl_ind])\n",
    "\n",
    "print(sampl_ind[ind[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1],\n",
       "       [ 2,  1],\n",
       "       [10, -2],\n",
       "       [ 1,  0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[1, 1], [2, 1]])\n",
    "test_y = np.array([2, 4])\n",
    "\n",
    "y_labels = np.array([1, 2, 3])\n",
    "y_proba = np.array([[1, 0], [111, 1], [10, -2]])\n",
    "y_proba = entropy(y_proba, axis=1)\n",
    "ind = np.lexsort((y_proba, y_proba))\n",
    "print(ind)\n",
    "\n",
    "y_proba = np.array([[1, 0], [111, 1], [10, -2]])\n",
    "\n",
    "test = np.append(test, y_proba[ind[0:2]], axis=0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "sug = Suggester(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Looking at 8102 samples from test.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.7792310653056544 8.090449338436718e-13\n"
     ]
    }
   ],
   "source": [
    "suggest_alg = EntropySampling(100)\n",
    "sug.active_learning_suggest(suggest_alg, LinearModel(model), sample_ratio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([263663, 556963, 162239,  32687, 534678, 123910, 376963, 676468,\n",
       "       232583, 569997, 102716, 393583, 645799, 105063, 738217, 767381,\n",
       "       723656, 759143, 378407, 476941,  88606, 138546, 295977, 376829,\n",
       "       185133, 183306, 299197, 167503, 128336, 406411, 341994, 229912,\n",
       "       322558, 309247, 102168, 161123, 167053, 782591, 624216, 293301,\n",
       "       495366, 319562, 602956,   5770, 205171, 627581, 189452, 194699,\n",
       "       532958,   8917, 637468, 141753, 375895, 306330, 312546, 410753,\n",
       "        36320, 772195, 399059, 795146, 142815, 580262, 405912,  42023,\n",
       "       635218,   4919, 788857, 644924, 772022, 780991, 498361, 464755,\n",
       "       586466,  40846, 517110, 786113, 320149, 306214, 610751, 782525,\n",
       "       162965, 274700,  36375, 181143, 406103, 693918, 799682, 768447,\n",
       "       142749,  46536, 604493,  93185, 282442, 147943, 483390, 623975,\n",
       "       197345, 522718, 629541, 210976])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sug.last_suggest.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 300), (202800, 300), (810200, 300))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_to_move = sug.X_test[sug.last_suggest.indices]\n",
    "samples_to_move.shape, sug.X_train.shape, sug.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug.apply_last_suggest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202900, 300), (810100, 300))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sug.X_train.shape, sug.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoLabeling(ActiveLearningBase):\n",
    "    def __init__(self, n_top=1000):\n",
    "        self.n_top = n_top\n",
    "\n",
    "    def get_samples_for_labeling(self, model, X_test, y_test):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        max_ind = np.argmax(y_proba, axis=1)\n",
    "        y_proba = np.max(y_proba, axis=1)\n",
    "        ind = np.lexsort((max_ind, y_proba))[::-1]\n",
    "        ind_to_return = ind[:min(self.n_top, y_proba.shape[0])]\n",
    "        return \"relabeling\", ind_to_return, max_ind[ind_to_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Looking at 8104 samples from test.\n"
     ]
    }
   ],
   "source": [
    "suggest_alg = PseudoLabeling(100)\n",
    "sug.active_learning_suggest(suggest_alg, LinearModel(model), sample_ratio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_move = sug.X_test[sug.last_suggest.indices]\n",
    "samples_to_move.shape, sug.X_train.shape, sug.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is no new unapplied suggests available. Aborting.\n"
     ]
    }
   ],
   "source": [
    "sug.apply_last_suggest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark our solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_output_path = \"selfpost/vectorized.npy\"\n",
    "vectorized_labels_output_path = \"selfpost/vectorized_labels.npy\"\n",
    "with open(vectorized_output_path, \"rb\") as vect_X, open(vectorized_labels_output_path, \"rb\") as vect_y:\n",
    "    X = np.load(vect_X, allow_pickle=True)\n",
    "    y = np.load(vect_y, allow_pickle=True)\n",
    "model_path = \"selfpost/models/logreg_100it\"\n",
    "model = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Looking at 81040 samples from test.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5454084402764067, 'f1_score': 0.5411580561102263, 'precision_score': 0.5616867560333363, 'recall_score': 0.5460126540293783}\n",
      "(810400, 300) (810400,)\n",
      "(800400, 300) (800400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 29.2min finished\n",
      "INFO:root:Looking at 80040 samples from test.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.546228135932034, 'f1_score': 0.5417091029620242, 'precision_score': 0.5615681180355546, 'recall_score': 0.5465371881251833}\n",
      "(790400, 300) (790400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "sug = Suggester(X,y)\n",
    "suggest_alg = ActiveLearning.EntropySampling(10000)\n",
    "epochs = 5\n",
    "print(sug.evaluate_metrics(LinearModel(model)))\n",
    "print(sug.X_test.shape, sug.y_test.shape)\n",
    "for ep in range(epochs):\n",
    "    sug.active_learning_suggest(suggest_alg, LinearModel(model), sample_ratio=0.1)\n",
    "    sug.apply_last_suggest()\n",
    "    print(sug.X_test.shape, sug.y_test.shape)\n",
    "    model.fit(sug.X_train, sug.y_train)\n",
    "    print(sug.evaluate_metrics(LinearModel(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually took too long to train logreg from sklearn. I am going to implement one with pytorch to train on cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "\n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(300, 1013),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LinearModelTorch(ModelWrap):\n",
    "    def __init__(self, model, iter):\n",
    "        self.model = model\n",
    "        self.last_loss = 0\n",
    "        self.iter = iter\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        X = torch.from_numpy(X)\n",
    "        y = torch.from_numpy(y)\n",
    "        X = X.view(-1, 300)\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        self.model = self.model.cuda()\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        criterion = nn.NLLLoss()\n",
    "        for epoch in range(self.iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(X.float())\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (epoch % 10 == 0):\n",
    "                print(f\"ep{epoch}: {loss}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.view(-1, 300)\n",
    "        X = X.cpu()\n",
    "        self.model = self.model.cpu()\n",
    "        self.model = self.model.eval()\n",
    "        return torch.argmax(self.model(X.float()), axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.view(-1, 300)\n",
    "        X = X.cpu()\n",
    "        return torch.exp(self.model(X.float()))\n",
    "    \n",
    "    def get_last_loss(self, X_train, y_train):\n",
    "        return log_loss(y_train, self.model.predict_proba(X_train), eps=1e-15)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "ep0: 6.9341301918029785\n",
      "ep10: 6.924700736999512\n",
      "ep20: 6.916568279266357\n",
      "ep30: 6.909415245056152\n",
      "ep40: 6.90291166305542\n",
      "ep50: 6.896817207336426\n",
      "ep60: 6.89097785949707\n",
      "ep70: 6.88529109954834\n",
      "ep80: 6.879701614379883\n",
      "ep90: 6.874173164367676\n"
     ]
    }
   ],
   "source": [
    "model = LinearModelTorch(LogReg(), 100)\n",
    "print(y_train.dtype)\n",
    "model.train(X_train, y_train.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"selfpost/models/logreg_torch1000it\"\n",
    "torch.save(model.model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"selfpost/models/logreg_torch1000it\"\n",
    "model = LogReg()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "model = LinearModelTorch(model, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPAPEC\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "INFO:root:Looking at 81040 samples from test.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.00846248766041461, 'f1_score': 0.003575499436051318, 'precision_score': 0.029538474314824134, 'recall_score': 0.008681346212082577}\n",
      "(810400, 300) (810400,)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-940c45fe4a23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive_learning_suggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuggest_alg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_last_suggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\repos\\coursework\\modules\\Suggester.py\u001b[0m in \u001b[0;36mactive_learning_suggest\u001b[1;34m(self, suggest_algorithm, model, apply, sample_ratio)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Looking at {num_of_elements} samples from test.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0msuggest_output\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0msuggest_algorithm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_samples_for_labeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msuggest_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"oracle\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msuggest_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\repos\\coursework\\modules\\ActiveLearning.py\u001b[0m in \u001b[0;36mget_samples_for_labeling\u001b[1;34m(self, model, X_test, y_test)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_samples_for_labeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0my_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0my_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_proba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"oracle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_top\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_proba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36mentropy\u001b[1;34m(pk, qk, base, axis)\u001b[0m\n\u001b[0;32m   2683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m     \"\"\"\n\u001b[1;32m-> 2685\u001b[1;33m     \u001b[0mpk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2686\u001b[0m     \u001b[0mpk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpk\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mqk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "sug = Suggester(X,y)\n",
    "suggest_alg = ActiveLearning.EntropySampling(10000)\n",
    "epochs = 5\n",
    "print(sug.evaluate_metrics(model))\n",
    "print(sug.X_test.shape, sug.y_test.shape)\n",
    "for ep in range(epochs):\n",
    "    sug.active_learning_suggest(suggest_alg, model, sample_ratio=0.1)\n",
    "    sug.apply_last_suggest()\n",
    "    print(sug.X_test.shape, sug.y_test.shape)\n",
    "    model.train(sug.X_train, sug.y_train)\n",
    "    print(sug.evaluate_metrics(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
