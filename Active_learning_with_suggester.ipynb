{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TOPAPEC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TOPAPEC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import faiss\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import modules\n",
    "import pickle\n",
    "import sys\n",
    "import numpy\n",
    "import logging\n",
    "import modules.ActiveLearning as ActiveLearning\n",
    "import modules.preprocess as preprocess\n",
    "from modules.ModelWrap import ModelWrap\n",
    "from modules.ActiveLearningBase import ActiveLearningBase\n",
    "from modules.Suggester import Suggester\n",
    "from modules.Suggest import Suggest\n",
    "importlib.reload(modules)\n",
    "importlib.reload(modules.Suggest)\n",
    "importlib.reload(modules.Suggester)\n",
    "importlib.reload(modules.ActiveLearningBase)\n",
    "importlib.reload(modules.ActiveLearning)\n",
    "importlib.reload(preprocess)\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "from multiprocessing import Pool\n",
    "from sklearn import preprocessing\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=100000)\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPAPEC\\anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# This time we are using reddit self-post classification dataset https://www.kaggle.com/mswarbrickjones/reddit-selfposts/?select=subreddit_info.csv\n",
    "# We will assume that post title and post body can be concatenated into one text column.\n",
    "embeddings_dim = 300\n",
    "tqdm.pandas()\n",
    "\n",
    "def preprocess_and_save_dataset(dataset_path, preprocessed_path):\n",
    "    dataframe = pd.read_csv(dataset_path, sep='\\t')\n",
    "    dataframe.loc[:, \"text\"] = dataframe.loc[:, \"title\"] + \" \" + dataframe.loc[:, \"selftext\"]\n",
    "    dataframe = dataframe.drop([\"title\", \"selftext\"], axis=1)\n",
    "    preprocess_pipeline(dataframe)\n",
    "    dataframe.to_pickle(preprocessed_path)\n",
    "    del dataframe\n",
    "\n",
    "def vectorize_and_save_dataset(pickle_path, output_path, output_path_labels):\n",
    "    dataset = pd.read_pickle(pickle_path)\n",
    "    vectorized = np.zeros((dataset.shape[0], embeddings_dim))\n",
    "    embeddings = get_glove_reddit_embeddings()\n",
    "    for i, (vec, row) in enumerate(zip(vectorized, dataset.iterrows())):\n",
    "        vectorized[i] = preprocess.row_to_embedding(row, embeddings, embeddings_dim)\n",
    "    del embeddings\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        np.save(file, vectorized)\n",
    "    with open(output_path_labels, \"wb\") as file:\n",
    "        np.save(file, dataset.loc[:, \"subreddit\"].to_numpy())\n",
    "    del dataset\n",
    "\n",
    "    \n",
    "def preprocess_pipeline(dataset):\n",
    "    cores = 12\n",
    "    multicore_tok(dataset, cores)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    dataset.loc[:, \"text\"].progress_apply(lemmatize_sent, args=[wnl])\n",
    "\n",
    "def lemmatize_sent(wordlist, wnl):\n",
    "    return ' '.join([wnl.lemmatize(w) for w in wordlist])\n",
    "    \n",
    "def multicore_tok(dataset, cores=6):\n",
    "    with Pool(processes=cores) as pool:\n",
    "        dataset.loc[:, \"text\"] = pool.map(nltk.word_tokenize, dataset.loc[:, \"text\"])\n",
    "\n",
    "def multicore_lem(dataset, cores=6):\n",
    "    with Pool(processes=cores) as pool:\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for i, line in tqdm(enumerate(dataset.text)):\n",
    "            dataset.loc[i,\"text\"] = pool.map(wnl.lemmatize, dataset.loc[i, \"text\"])\n",
    "            \n",
    "def get_glove_reddit_embeddings():\n",
    "    # Number of words - 1623397 \n",
    "    embeddings = {}\n",
    "    tmp = []\n",
    "    with io.open(\"GloVe.Reddit.120B.300D.txt\", \"r\", encoding='utf-8') as file:\n",
    "        file.readline()\n",
    "        for line in tqdm(file, total=1623397):\n",
    "            tmp.append(line)\n",
    "    with Pool(processes=14) as pool:\n",
    "        tmp = list(tqdm(pool.imap(preprocess.fetch_embeddings_value, tmp, chunksize=200000), total=1623397))\n",
    "    for word, vector in tqdm(tmp):\n",
    "        embeddings[word] = vector\n",
    "    del tmp\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"selfpost/rspct.tsv\"\n",
    "preprocessed_path = \"selfpost/preprocessed.pkl\"\n",
    "preprocess_and_save_dataset(dataset_path, preprocessed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_output_path = \"selfpost/vectorized.npy\"\n",
    "vectorized_labels_output_path = \"selfpost/vectorized_labels.npy\"\n",
    "vectorize_and_save_dataset(preprocessed_path, vectorized_output_path, vectorized_labels_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "class LinearModel(ModelWrap):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.last_loss = 0\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        self.model.train(X,y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def get_last_loss(self, X_train, y_train):\n",
    "        return log_loss(y_train, model.predict_proba(X_train), eps=1e-15)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "class ConfidenceSamplingSuggestion(ActiveLearningBase):\n",
    "    def __init__(self, n_top=1000):\n",
    "        self.n_top = n_top\n",
    "\n",
    "    def get_samples_for_labeling(self, model, X_test, y_test):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        y_proba = np.max(y_proba, axis=1)\n",
    "        ind = np.lexsort((y_test, y_proba))\n",
    "        return \"oracle\", ind[:min(self.n_top, y_proba.shape[0])]\n",
    "\n",
    "class MarginSampling(ActiveLearningBase):\n",
    "    def __init__(self, n_top=1000):\n",
    "        self.n_top = n_top\n",
    "        \n",
    "    def get_samples_for_labeling(self, model, X_test, y_test):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        y_proba = np.sort(y_proba, axis=1)[:,::-1]\n",
    "        y_proba = y_proba[:,0] - y_proba[:,1]\n",
    "        ind = np.lexsort((y_test, y_proba))\n",
    "        return \"oracle\", ind[:min(self.n_top, y_proba.shape[0])]\n",
    "\n",
    "class EntropySampling(ActiveLearningBase):\n",
    "    def __init__(self, n_top=1000):\n",
    "        self.n_top = n_top\n",
    "        \n",
    "    def get_samples_for_labeling(self, model, X_test, y_test):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        y_proba = entropy(y_proba, axis=1)\n",
    "        ind = np.lexsort((y_test, y_proba))[::-1]\n",
    "        print(y_proba[ind[0]], y_proba[ind[-1]])\n",
    "        return \"oracle\", ind[:min(self.n_top, y_proba.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "vectorized_output_path = \"selfpost/vectorized.npy\"\n",
    "vectorized_labels_output_path = \"selfpost/vectorized_labels.npy\"\n",
    "with open(vectorized_output_path, \"rb\") as vect_X, open(vectorized_labels_output_path, \"rb\") as vect_y:\n",
    "    X = np.load(vect_X, allow_pickle=True)\n",
    "    y = np.load(vect_y, allow_pickle=True)\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "del le\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2fb12d4e3baf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"selfpost/models/logreg_100it\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = \"selfpost/models/logreg_100it\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_seed)\n",
    "model = LogisticRegression(random_state=random_seed, n_jobs=-1, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "dump(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.06228423,  0.06092767, -0.14749891, ..., -0.3225482 ,\n",
       "          0.14105452,  0.00854856],\n",
       "        [ 0.08068533, -0.02982698, -0.12298278, ..., -0.27350412,\n",
       "          0.05903656,  0.0867618 ],\n",
       "        [ 0.03988495,  0.06417522, -0.20442137, ..., -0.29071138,\n",
       "          0.03378086, -0.01860267],\n",
       "        ...,\n",
       "        [ 0.15182757,  0.10873834, -0.17133273, ..., -0.17019229,\n",
       "          0.15758706,  0.04102596],\n",
       "        [ 0.04931326,  0.03915333, -0.15517242, ..., -0.30393526,\n",
       "          0.0845803 , -0.00351562],\n",
       "        [ 0.09860376,  0.01065233, -0.13773885, ..., -0.28388423,\n",
       "          0.1883342 , -0.11135513]]),\n",
       " array([470, 445, 853, ...,   9, 288, 667]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"selfpost/models/logreg_100it\"\n",
    "model = load(model_path)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_seed)\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrappedModel = LinearModel(model)\n",
    "confSuggest = ConfidenceSamplingSuggestion(10000)\n",
    "sampl_ind = rng.choice(X_test.shape[0], 1000, replace=False)\n",
    "ind = confSuggest.get_samples_for_labeling(model, X_test[sampl_ind, :], y_test[sampl_ind])\n",
    "\n",
    "print(sampl_ind[ind[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1],\n",
       "       [ 2,  1],\n",
       "       [10, -2],\n",
       "       [ 1,  0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[1, 1], [2, 1]])\n",
    "test_y = np.array([2, 4])\n",
    "\n",
    "y_labels = np.array([1, 2, 3])\n",
    "y_proba = np.array([[1, 0], [111, 1], [10, -2]])\n",
    "y_proba = entropy(y_proba, axis=1)\n",
    "ind = np.lexsort((y_proba, y_proba))\n",
    "print(ind)\n",
    "\n",
    "y_proba = np.array([[1, 0], [111, 1], [10, -2]])\n",
    "\n",
    "test = np.append(test, y_proba[ind[0:2]], axis=0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "sug = Suggester(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Looking at 8102 samples from test.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.7792310653056544 8.090449338436718e-13\n"
     ]
    }
   ],
   "source": [
    "suggest_alg = EntropySampling(100)\n",
    "sug.active_learning_suggest(suggest_alg, LinearModel(model), sample_ratio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([263663, 556963, 162239,  32687, 534678, 123910, 376963, 676468,\n",
       "       232583, 569997, 102716, 393583, 645799, 105063, 738217, 767381,\n",
       "       723656, 759143, 378407, 476941,  88606, 138546, 295977, 376829,\n",
       "       185133, 183306, 299197, 167503, 128336, 406411, 341994, 229912,\n",
       "       322558, 309247, 102168, 161123, 167053, 782591, 624216, 293301,\n",
       "       495366, 319562, 602956,   5770, 205171, 627581, 189452, 194699,\n",
       "       532958,   8917, 637468, 141753, 375895, 306330, 312546, 410753,\n",
       "        36320, 772195, 399059, 795146, 142815, 580262, 405912,  42023,\n",
       "       635218,   4919, 788857, 644924, 772022, 780991, 498361, 464755,\n",
       "       586466,  40846, 517110, 786113, 320149, 306214, 610751, 782525,\n",
       "       162965, 274700,  36375, 181143, 406103, 693918, 799682, 768447,\n",
       "       142749,  46536, 604493,  93185, 282442, 147943, 483390, 623975,\n",
       "       197345, 522718, 629541, 210976])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sug.last_suggest.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 300), (202800, 300), (810200, 300))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_to_move = sug.X_test[sug.last_suggest.indices]\n",
    "samples_to_move.shape, sug.X_train.shape, sug.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug.apply_last_suggest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202900, 300), (810100, 300))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sug.X_train.shape, sug.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoLabeling(ActiveLearningBase):\n",
    "    def __init__(self, n_top=1000):\n",
    "        self.n_top = n_top\n",
    "\n",
    "    def get_samples_for_labeling(self, model, X_test, y_test):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        max_ind = np.argmax(y_proba, axis=1)\n",
    "        y_proba = np.max(y_proba, axis=1)\n",
    "        ind = np.lexsort((max_ind, y_proba))[::-1]\n",
    "        ind_to_return = ind[:min(self.n_top, y_proba.shape[0])]\n",
    "        return \"relabeling\", ind_to_return, max_ind[ind_to_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Looking at 8104 samples from test.\n"
     ]
    }
   ],
   "source": [
    "suggest_alg = PseudoLabeling(100)\n",
    "sug.active_learning_suggest(suggest_alg, LinearModel(model), sample_ratio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_move = sug.X_test[sug.last_suggest.indices]\n",
    "samples_to_move.shape, sug.X_train.shape, sug.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is no new unapplied suggests available. Aborting.\n"
     ]
    }
   ],
   "source": [
    "sug.apply_last_suggest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark our solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_output_path = \"selfpost/vectorized.npy\"\n",
    "vectorized_labels_output_path = \"selfpost/vectorized_labels.npy\"\n",
    "with open(vectorized_output_path, \"rb\") as vect_X, open(vectorized_labels_output_path, \"rb\") as vect_y:\n",
    "    X = np.load(vect_X, allow_pickle=True)\n",
    "    y = np.load(vect_y, allow_pickle=True)\n",
    "model_path = \"selfpost/models/logreg_100it\"\n",
    "model = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Looking at 81040 samples from test.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5454084402764067, 'f1_score': 0.5411580561102263, 'precision_score': 0.5616867560333363, 'recall_score': 0.5460126540293783}\n",
      "(810400, 300) (810400,)\n",
      "(800400, 300) (800400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 29.2min finished\n",
      "INFO:root:Looking at 80040 samples from test.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.546228135932034, 'f1_score': 0.5417091029620242, 'precision_score': 0.5615681180355546, 'recall_score': 0.5465371881251833}\n",
      "(790400, 300) (790400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "sug = Suggester(X,y)\n",
    "suggest_alg = ActiveLearning.EntropySampling(10000)\n",
    "epochs = 5\n",
    "print(sug.evaluate_metrics(LinearModel(model)))\n",
    "print(sug.X_test.shape, sug.y_test.shape)\n",
    "for ep in range(epochs):\n",
    "    sug.active_learning_suggest(suggest_alg, LinearModel(model), sample_ratio=0.1)\n",
    "    sug.apply_last_suggest()\n",
    "    print(sug.X_test.shape, sug.y_test.shape)\n",
    "    model.fit(sug.X_train, sug.y_train)\n",
    "    print(sug.evaluate_metrics(LinearModel(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually took too long to train logreg from sklearn. I am going to implement one with pytorch to train on cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(300, 1013),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "\n",
    "def worker_init_fn(x):\n",
    "    seed = args.seed + x\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    return\n",
    "\n",
    "\n",
    "class LinearModelTorch(ModelWrap):\n",
    "    def __init__(self, model, itr):\n",
    "        self.model = model\n",
    "        self.last_loss = 0\n",
    "        self.iter = itr\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        torch.cuda.empty_cache()\n",
    "        X = torch.from_numpy(X.astype(np.float32))\n",
    "        y = torch.from_numpy(y.astype(np.int64))\n",
    "        self.model = self.model.cuda()\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "        criterion = nn.NLLLoss()\n",
    "        dataset = TensorDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=10000, num_workers=0)\n",
    "        for epoch in tqdm(range(self.iter)):\n",
    "            for bX, by in dataloader:\n",
    "                bX = bX.view(-1, 300)\n",
    "                bX = bX.cuda()\n",
    "                by = by.cuda()\n",
    "                torch.cuda.manual_seed(random_seed)\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.manual_seed(random_seed)\n",
    "                output = self.model(bX)\n",
    "                loss = criterion(output, by)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             if (epoch % 10 == 0):\n",
    "#                 print(f\"ep{epoch}: {loss}\")\n",
    "            torch.cuda.empty_cache()\n",
    "        X = X.cpu().detach().numpy()\n",
    "        y = y.cpu().detach().numpy()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.view(-1, 300)\n",
    "        X = X.cpu()\n",
    "        self.model = self.model.cpu()\n",
    "        self.model = self.model.eval()\n",
    "        result = torch.argmax(self.model(X.float()), axis=1)\n",
    "        X = X.detach().numpy()\n",
    "        return result.detach().numpy()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.from_numpy(X)\n",
    "        X = X.view(-1, 300)\n",
    "        X = X.cpu()\n",
    "        result = torch.exp(self.model(X.float()))\n",
    "        X = X.detach().numpy()\n",
    "        return result.detach().numpy()\n",
    "    \n",
    "    def get_last_loss(self, X_train, y_train):\n",
    "        return log_loss(y_train, self.model.predict_proba(X_train), eps=1e-15)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "random.seed(random_seed)\n",
    "\n",
    "model = LinearModelTorch(LogReg(), 100)\n",
    "model.train(X_train, y_train.astype(np.int64))\n",
    "# print(sug.evaluate_metrics(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"selfpost/models/logreg_torch1000it\"\n",
    "torch.save(model.model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"selfpost/models/logreg_torch1000it\"\n",
    "model = LogReg()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "model = LinearModelTorch(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load training data by batches.\n",
    "from torch.utils.data import IterableDataset\n",
    "class IterDataset(IterableDataset):\n",
    "    def __init__(self, filename, chunksize):\n",
    "        self.filename = filename\n",
    "        self.chunksize = chunksize\n",
    "        \n",
    "    def __iter__(self):\n",
    "        csv = pd.read_csv(self.filename, chunksize=self.chunksize)\n",
    "        for chunk in csv:\n",
    "            y = chunk[\"label\"]\n",
    "            X = chunk[\"features\"]\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:22<00:00,  2.03s/it]\n",
      "INFO:root:Looking at 962350 samples from test.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5013633293500286, 'f1_score': 0.4941436594383415, 'precision_score': 0.583224394126391, 'recall_score': 0.5018772453211574}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████████████████████████████████████████████████▉                         | 69/100 [00:48<00:21,  1.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-8576b45b9a9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_last_suggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearModelTorch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-4bcd0007e4e7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 \u001b[0mbX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mbX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "random.seed(random_seed)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "sug = Suggester(X,y, test_size=0.95)\n",
    "suggest_alg = ActiveLearning.EntropySampling(20000)\n",
    "epochs = 5\n",
    "model = LinearModelTorch(LogReg(), 100)\n",
    "model.train(X_train, y_train.astype(np.int64))\n",
    "print(sug.evaluate_metrics(model))\n",
    "for ep in range(epochs):\n",
    "    sug.active_learning_suggest(suggest_alg, model, sample_ratio=1)\n",
    "    sug.apply_last_suggest()\n",
    "    model = LinearModelTorch(LogReg(), 100)\n",
    "    model.train(sug.X_train, sug.y_train)\n",
    "    print(sug.evaluate_metrics(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainIndistinguishable(ActiveLearningBase):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def build_confusion_map(self, y_pred, y_train):\n",
    "#         {\"a-b\":100, \"b-c\":10, ...}\n",
    "        pass\n",
    "    \n",
    "    def get_samples_for_labeling(self, model, X_train, y_train):\n",
    "        logging.Info(\"Class a and b were confused 1000 times in total. To relabel them to class a-b apply this suggest...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
