{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for embeddings\n",
    "Let's scrape from 1000 to 1500 comments for each initial dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TOPAPEC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TOPAPEC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import lxml\n",
    "import cchardet\n",
    "import time\n",
    "import numpy as np\n",
    "import io\n",
    "import regex as re\n",
    "import importlib\n",
    "import gensim\n",
    "import modules.preprocess as preprocess\n",
    "importlib.reload(preprocess)\n",
    "import functools\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from scipy import spatial\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from nltk import WordNetLemmatizer\n",
    "from multiprocessing import Pool\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle datasets download -d theshadow29/subreddit-classification\n",
    "try:\n",
    "    os.mkdir(\"dataset\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory already exists\")\n",
    "kaggle.api.authenticate()\n",
    "kaggle.api.dataset_download_files(\"theshadow29/subreddit-classification\", path=\"dataset\", unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset/fine_grained_full.csv')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = int(dt.datetime(2017, 1, 1, 0, 0).timestamp())\n",
    "end_date = int(dt.datetime(2019, 1, 1, 0, 0).timestamp())\n",
    "limit_per_subreddit = 1500\n",
    "pages_per_sub = 40\n",
    "subreddits = dataset[\"label\"].unique()\n",
    "try:\n",
    "    os.mkdir(\"reddit_comments\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory already exists\")\n",
    "subreddits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_page(subreddit):\n",
    "    titles = []\n",
    "    url = f\"https://old.reddit.com/r/{subreddit}/\"\n",
    "    requests_session = requests.Session()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    for i in range(pages_per_sub):\n",
    "        if (i % 5 == 4):\n",
    "            time.sleep(2)\n",
    "        page = requests_session.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        attrs = {'class': 'thing'}\n",
    "        for post in soup.find_all('div', attrs=attrs):\n",
    "            titles.append(post.find('p', class_=\"title\").text)\n",
    "        next_button = soup.find(\"span\", class_=\"next-button\")\n",
    "        try:\n",
    "            url = next_button.find(\"a\").attrs['href']\n",
    "        except:\n",
    "            print(f\"{subreddit} page {i}\")\n",
    "            break\n",
    "        \n",
    "    return (subreddit, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = dt.datetime.now()\n",
    "for chunk in tqdm(range(0, 1401, 40)):\n",
    "    with Pool(12) as pool:\n",
    "        for subreddit, result in pool.map(pull_page, subreddits[chunk:min(chunk + 40, 1430)]):\n",
    "            df = pd.DataFrame(result)\n",
    "            df.to_csv(f\"reddit_comments{os.path.sep}{subreddit}.csv\", index=False)\n",
    "        time.sleep(2)\n",
    "passed = dt.datetime.now() - start\n",
    "print(passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pull_page(subreddits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I failed to parse subreddits as there were too little titles available and the data was too noisy as well. I managed to parse 90 mib of data but of course that wasn't nearly enough to train decent embedding. At the same time my attempts to use pushshift api failed as there was a problem on their side (api returned only super small fraction of each query. \n",
    "\n",
    "But luckily I found pretrained glove word embeddings trained on more than 250 gib of reddit data. So I'm willing to test if they are good enough. \n",
    "https://www.kaggle.com/leighplt/glove-reddit-comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_reddit_embeddings():\n",
    "    # Number of words - 1623397 \n",
    "    embeddings = {}\n",
    "    tmp = []\n",
    "    with io.open(\"GloVe.Reddit.120B.300D.txt\", \"r\", encoding='utf-8') as file:\n",
    "        file.readline()\n",
    "        for line in tqdm(file, total=1623397):\n",
    "            tmp.append(line)\n",
    "    with Pool(processes=14) as pool:\n",
    "        tmp = list(tqdm(pool.imap(preprocess.fetch_embeddings_value, tmp, chunksize=200000), total=1623397))\n",
    "    for word, vector in tqdm(tmp):\n",
    "        embeddings[word] = vector\n",
    "    del tmp\n",
    "    return embeddings\n",
    "\n",
    "def get_word2vec_embeddings():\n",
    "    return gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def get_fasttext_embeddings():\n",
    "    return load_facebook_vectors('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/leighplt/glove-reddit-comments\n",
    "\n",
    "def test_embeddings_wordsim(embeddings):\n",
    "    av_abs_dev = 0.0\n",
    "    file_len = sum(1 for line in open('wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt'))\n",
    "    with open(\"wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt\") as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            print(values[0], values[1], values[2])\n",
    "            simil = float(values[2]) / 10.0\n",
    "            cos_sim = 1 - spatial.distance.cosine(embeddings[values[0]], embeddings[values[1]])\n",
    "            print(f\"Accordings to embeddings {cos_sim}\")\n",
    "            av_abs_dev += abs(cos_sim - simil)\n",
    "    print(f\"Final average abs deviation: {av_abs_dev / float(file_len)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings_wordsim(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings_wordsim(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see wordsim test is not very representative of the quality of embeddings. Now, let's use classic ml classifiers on our dataset to determine the best vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(429300, 2) (1000156, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset1 = pd.read_csv('dataset/fine_grained_full.csv')\n",
    "dataset2 = pd.read_csv('dataset/cleaned_all_title_data_controversial.csv')\n",
    "print(dataset1.shape, dataset2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>studyroomf</td>\n",
       "      <td>Do you subscribe to the theory that all the ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studyroomf</td>\n",
       "      <td>A pivotal moment for the dean: \"We love you too\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>studyroomf</td>\n",
       "      <td>Episode Discussion - S04E05 - Cooperative Esca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>studyroomf</td>\n",
       "      <td>Dan Harmon says \"There's a character from seas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>studyroomf</td>\n",
       "      <td>'Can we take a sidebar from this sidebar?' Sug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0  studyroomf  Do you subscribe to the theory that all the ev...\n",
       "1  studyroomf   A pivotal moment for the dean: \"We love you too\"\n",
       "2  studyroomf  Episode Discussion - S04E05 - Cooperative Esca...\n",
       "3  studyroomf  Dan Harmon says \"There's a character from seas...\n",
       "4  studyroomf  'Can we take a sidebar from this sidebar?' Sug..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_pipeline(dataset):\n",
    "    cores = 12\n",
    "    multicore_tok(dataset, cores)\n",
    "    multicore_lem(dataset, cores)\n",
    "\n",
    "def multicore_tok(dataset, cores=6):\n",
    "    with Pool(processes=cores) as pool:\n",
    "        dataset.loc[:, \"text\"] = pool.map(nltk.word_tokenize, dataset.loc[:, \"text\"])\n",
    "\n",
    "def multicore_lem(dataset, cores=6):\n",
    "    with Pool(processes=cores) as pool:\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for i, line in tqdm(enumerate(dataset.text)):\n",
    "            dataset.loc[i,\"text\"] = pool.map(wnl.lemmatize, dataset.loc[i, \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236e4b46807743c9a3cce24c3bea6fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        label                                               text\n",
      "0  studyroomf  [Do, you, subscribe, to, the, theory, that, al...\n",
      "1  studyroomf  [A, pivotal, moment, for, the, dean, :, ``, We...\n",
      "2  studyroomf  [Episode, Discussion, -, S04E05, -, Cooperativ...\n",
      "3  studyroomf  [Dan, Harmon, say, ``, There, 's, a, character...\n"
     ]
    }
   ],
   "source": [
    "preprocess_pipeline(dataset1)\n",
    "print(dataset1[:4])\n",
    "dataset1.to_csv(\"preprocessed_serialised/dataset_fine_grained_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline(dataset2)\n",
    "print(dataset2[:4])\n",
    "dataset2.to_csv(\"preprocessed_serialised/dataset_cleaned_all_title_data_controversial.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPAPEC\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "dateset_controversial = preprocess.parse_lemmatized(\"preprocessed_serialised/dataset_cleaned_all_title_data_controversial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1000156/1000156 [00:08<00:00, 121618.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# import multiprocessing, logging\n",
    "# logger = multiprocessing.log_to_stderr()\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "dateset_controversial.loc[:, \"text\"] = preprocess.clean_further(dateset_controversial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display.max_colwidth : int or None\n",
      "    The maximum width in characters of a column in the repr of\n",
      "    a pandas data structure. When the column overflows, a \"...\"\n",
      "    placeholder is embedded in the output. A 'None' value means unlimited.\n",
      "    [default: 50] [currently: 300]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('max_rows', 1000)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "pd.describe_option('max_colwidth')\n",
    "\n",
    "# dateset_controversial.sample(frac=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply classes-tfidf with word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_features(dataset, cores=12):\n",
    "    X_reduced = pd.DataFrame()\n",
    "    X_reduced.loc[:, \"label\"] = dataset[\"label\"].unique()\n",
    "    X_reduced[\"text\"] = [[] for i in range(X_reduced.shape[0])]\n",
    "    for i, label in enumerate(tqdm(X_reduced.loc[:, \"label\"])):\n",
    "        titles = dataset[dataset.label == label][\"text\"].to_list()\n",
    "        X_reduced.loc[i, \"text\"].extend(word for title in titles for word in title)\n",
    "    with Pool(processes=cores) as pool:\n",
    "        tmp = list(tqdm(pool.imap(preprocess.unite_string, X_reduced.loc[:,\"text\"], chunksize=(X_reduced.shape[0] // 100)), total=X_reduced.shape[0]))\n",
    "        X_reduced.loc[:, \"text\"] = tmp\n",
    "    tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words='english')\n",
    "    tfidf_matrix = tfidfvectorizer.fit_transform(X_reduced.loc[:, \"text\"])\n",
    "    feature_names = tfidfvectorizer.vocabulary_\n",
    "    feature_names = {v: k for k, v in feature_names.items()}\n",
    "    return tfidf_matrix, feature_names\n",
    "\n",
    "def features_to_embeddings(dateset_controversial, ind, embeddings):\n",
    "    count = 0\n",
    "    result = np.zeros(300)\n",
    "    for word in x:\n",
    "        if word in embeddings:\n",
    "            count += 1\n",
    "            result += embeddings[word] * x\n",
    "            \n",
    "# def tfidf_features_to_embeddings_space(X, embeddings):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1466/1466 [00:47<00:00, 30.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1466/1466 [00:03<00:00, 434.58it/s]\n"
     ]
    }
   ],
   "source": [
    "matrix, names = get_features(dateset_controversial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2267670it [00:02, 918802.67it/s]\n"
     ]
    }
   ],
   "source": [
    "cx = matrix.tocoo()\n",
    "tfidf_dict = {}\n",
    "labels = dateset_controversial.loc[:, \"label\"].unique()\n",
    "for i,j,v in tqdm(zip(cx.row, cx.col, cx.data)):\n",
    "    if labels[i] not in tfidf_dict:\n",
    "        tfidf_dict[labels[i]] = {}\n",
    "    tfidf_dict[labels[i]][names[j]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1623397/1623397 [00:05<00:00, 296258.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1623397/1623397 [00:23<00:00, 68554.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1623397/1623397 [00:00<00:00, 2124390.68it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_glove_reddit_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7c690a116271>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow_to_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidf_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdateset_controversial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdateset_controversial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "with Pool(processes=14) as pool:\n",
    "    X = list(tqdm(pool.imap(functools.partial(preprocess.row_to_embedding, embeddings=embeddings, embeddings_dim=300, tfidf_dict=tfidf_dict), dateset_controversial.iterrows(), chunksize=200000), total=dateset_controversial.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dateset_controversial.loc[:, \"label\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 12.6min finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training\")\n",
    "model = LogisticRegression(random_state=random_seed, n_jobs=-1, verbose=True, max_iter=50)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32019996875488205\n",
      "0.3372055347877084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPAPEC\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5270011604440862\n",
      "0.29506955168567023\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(recall_score(y_test, y_pred, average=\"macro\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_tfidf_prod_embeddings(dataset, embeddings, embeddings_dim, tfidf_dict):\n",
    "    with Pool(processes=12) as pool:\n",
    "        return np.asarray(list(tqdm(pool.imap(functools.partial(preprocess.row_to_embedding, embeddings=embeddings, \n",
    "                                                  embeddings_dim=embeddings_dim, tfidf_dict=tfidf_dict), dataset.iterrows(), chunksize=20000), total=dataset.shape[0])))\n",
    "\n",
    "\n",
    "def transform_to_tfidf_prod_embeddings_linear(dataset, embeddings, embeddings_dim, tfidf_dict):\n",
    "    X = np.empty((dataset.shape[0], embeddings_dim), dtype=np.ndarray)\n",
    "    for i, line in tqdm(enumerate(dataset.iterrows()), total=dataset.shape[0]):\n",
    "        X[i] = functools.partial(preprocess.row_to_embedding, embeddings=embeddings, \n",
    "                                 embeddings_dim=embeddings_dim, tfidf_dict=tfidf_dict)(line)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dateset_controversial.loc[:, \"label\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "embeddings = get_word2vec_embeddings()\n",
    "X = transform_to_tfidf_prod_embeddings(dateset_controversial, embeddings, 300, tfidf_dict)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1000156/1000156 [04:16<00:00, 3898.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 13.0min finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training\")\n",
    "model = LogisticRegression(random_state=random_seed, n_jobs=-1, verbose=True, max_iter=50)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.207412591782534\n",
      "0.20976933841507145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPAPEC\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38519818557904995\n",
      "0.18745536394885665\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(recall_score(y_test, y_pred, average=\"macro\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1000156/1000156 [01:40<00:00, 9959.81it/s]\n"
     ]
    }
   ],
   "source": [
    "y = dateset_controversial.loc[:, \"label\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "embeddings = get_fasttext_embeddings()\n",
    "X = transform_to_tfidf_prod_embeddings_linear(dateset_controversial, embeddings, 300, tfidf_dict)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 12.9min finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training\")\n",
    "model = LogisticRegression(random_state=random_seed, n_jobs=-1, verbose=True, max_iter=50)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15277487892516795\n",
      "0.1439512317219167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPAPEC\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27191432800152043\n",
      "0.1338477866119142\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(recall_score(y_test, y_pred, average=\"macro\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove reddit vectors turned out to be the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
