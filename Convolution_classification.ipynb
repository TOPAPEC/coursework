{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Remember your command line switches... . Hi there,  <lb>The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn't the right place...<lb><lb>Alright. Here's the story. I'm an independent developer who produces my own software. We're going to call me well, $me.<lb><lb>I work with $dev who helps to produce software with me. We use $PopularVersionControl.<lb><lb>We're trying to remove a branch that was created by mistake.  The branch is beta1. We want just beta.<lb><lb>&gt; $me: \"$dev, can you rename that branch because we're going to use just two. I don't want to keep up with 80 quintilian branches.\"  <lb>&gt; $dev: \"sure, one second.\"<lb><lb>Five minutes later...<lb><lb>&gt; $dev: \"[CurseWords] I want beta1 to die!\"  <lb>&gt; $me: \"What happened?\"<lb><lb>Lots of removed dialog where $dev explains what he did...<lb><lb>&gt; $me: \"Did you try $PopularVersionControl with -u?\"  <lb>&gt; $dev: \"[Cursing] That would be why!\"<lb><lb>In short. Always check your command line switches...They are important!<lb>\n",
       "1    So what was Matt \"addicted\" to? . Did he ever say what his addiction was or is he still chugging beers while talking about how sober he is?<lb><lb>Edited to add: As an addict myself, anyone I know whose been an addict doesn't drink and in the group I go to (similar to NA and AA) drinking is considered a slip-up. Has he said what he's addicted to or is he still talking out of his ass, winging it as he goes?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2    No Club Colors . Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We always ended up at a bar called Hogs &amp; Heifers. It's worth noting the females working there can outdrink ANYONE. Anyway, there was a sign on the front door that read 'No Club Colors'. So we lose our ties and blazers before heading there. Also we assumed bright colors like red, yellow, green etc were not allowed. So we would always bring an xtra t-shirt and pair of jeans. This went on for years! Looking back now on how naive we were, it's just hilarious. I was never able to walk out of that bar....had to crawl out! So much booze. <lb><lb>Cheers. Ride safe, boys!                                                                                                                                                                                                                                                                \n",
       "3    Not door bell, but floodlight mount height. . I know this is a sub for the 'Ring Doorbell' but has anyone used the Floodlight?  I already have the wire and existing bracket for the floodlight on the back of my house, but the problem is that it's about 12 feet above ground level (10 ft above the deck, 2 ft drop from the deck down to the grass)<lb><lb>Is that too high to mount?  The website says 9 ft is ideal.  Anyone had any problems mounting it higher than that?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "dataset = pd.read_csv(\"selfpost/rspct.tsv\", sep=\"\\t\")\n",
    "dataset[\"text\"] = dataset[\"title\"] + \" . \" + dataset[\"selftext\"]\n",
    "dataset.loc[:3, \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load another file for bpe and concat it with dataset data.\n",
    "import requests\n",
    "corpus_url = \"https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\"\n",
    "r = requests.get(corpus_url) # create HTTP response object\n",
    "with open(\"botchan.txt\",'wb') as f:\n",
    "    f.write(r.content)\n",
    "with open(\"botchan.txt\", \"a\", encoding=\"utf-8\") as file: \n",
    "    for row in dataset[\"text\"]:\n",
    "        file.write(row + \". \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "\n",
    "\n",
    "def lemmatize_words(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [token.lemma_ for token in nlp(text)]\n",
    "\n",
    "def normalize_text(text):\n",
    "    return \" \".join([word for word in lemmatize_words(sp.decode_ids(sp.tokenize(text)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPAPEC\\anaconda3\\envs\\coursework\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=model --vocab_size=5239 --bos_id=-1 --pad_id=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "\n",
    "sp.load('model.model')\n",
    "\n",
    "# Я не уверен, насколько это хорошая практика, но я использовал sentencepiece, чтобы очистить текст от кучи склеенных слов, \n",
    "# а потом токенизировал nltk токенайзером (мне не очень понятно, как бить при помощи sentencepiece сразу на слова, а не члены его словаря)\n",
    "with open(f\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\dataset_normalized\\\\tokenized.pkl\", \"rb\") as file:\n",
    "    dataset = pickle.load(file) \n",
    "\n",
    "# dataset[\"text\"] = dataset[\"text\"].progress_apply(lambda x : sp.decode_ids(sp.tokenize(x)))\n",
    "\n",
    "# with open(\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\dataset_normalized\\\\tokenized.pkl\", \"wb\") as file: \n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "      <td>Remember your command line switch ... . Hi the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "      <td>So what wa Matt `` addicted '' to ? . Did he e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>No Club Colors . Funny story . I went to colle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "      <td>Not door bell , but floodlight mount height . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "      <td>Worried about my 8 ⁇ 00k small fft ⁇ data stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5qw3x0</td>\n",
       "      <td>residentevil</td>\n",
       "      <td>What if Saddler won?</td>\n",
       "      <td>I just wanted to start a thread about what wou...</td>\n",
       "      <td>What if Saddler won ? . I just wanted to start...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit  \\\n",
       "0  6d8knd  talesfromtechsupport   \n",
       "1  58mbft               teenmom   \n",
       "2  8f73s7                Harley   \n",
       "3  6ti6re          ringdoorbell   \n",
       "4  77sxto                 intel   \n",
       "5  5qw3x0          residentevil   \n",
       "\n",
       "                                               title  \\\n",
       "0             Remember your command line switches...   \n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "5                               What if Saddler won?   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...   \n",
       "1  Did he ever say what his addiction was or is h...   \n",
       "2  Funny story. I went to college in Las Vegas. T...   \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...   \n",
       "4  Prime95 (regardless of version) and OCCT both,...   \n",
       "5  I just wanted to start a thread about what wou...   \n",
       "\n",
       "                                                text  \n",
       "0  Remember your command line switch ... . Hi the...  \n",
       "1  So what wa Matt `` addicted '' to ? . Did he e...  \n",
       "2  No Club Colors . Funny story . I went to colle...  \n",
       "3  Not door bell , but floodlight mount height . ...  \n",
       "4  Worried about my 8 ⁇ 00k small fft ⁇ data stre...  \n",
       "5  What if Saddler won ? . I just wanted to start...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "# with open(\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\dataset_normalized\\\\lemmatized_after_tokenization.pkl\", \"rb\") as file:\n",
    "#     istart, dataset = pickle.load(file)\n",
    "# for i, item in enumerate(tqdm(nlp.pipe(dataset[\"text\"][istart:], n_process=8), total=dataset.shape[0])):\n",
    "#     dataset.loc[i, \"text\"] = item\n",
    "#     if i % 5000 == 4999:\n",
    "#         with open(\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\dataset_normalized\\\\lemmatized_after_tokenization.pkl\", \"wb\") as file: \n",
    "#             pickle.dump((i, dataset), file)\n",
    "\n",
    "# Todo pos_tag\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "# wnl = WordNetLemmatizer()\n",
    "# def lemmatize(text):\n",
    "#     return \" \".join([wnl.lemmatize(word) for word in nltk.word_tokenize(text)])\n",
    "\n",
    "# dataset[\"text\"] = dataset[\"text\"].progress_apply(lemmatize)\n",
    "\n",
    "# with open(\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\dataset_normalized\\\\lemmatized_after_tokenization.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(dataset, file)\n",
    "\n",
    "with open(\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\dataset_normalized\\\\lemmatized_after_tokenization.pkl\", \"rb\") as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Remember your command line switch ... . Hi there , ⁇ lb ⁇ The usual . Long time lerker , first time poster , be kind etc . Sorry if this is n't the right place ... ⁇ lb ⁇ lb ⁇ Alright . Here 's the story . I 'm an independent developer who produce my own software . We 're going to call me well , ⁇ me . ⁇ lb ⁇ lb ⁇ I work with ⁇ dev who help to produce software with me . We use ⁇ Popular ⁇ ersionControl . ⁇ lb ⁇ lb ⁇ We 're trying to remove a branch that wa created by mistake . The branch is beta1 . We want just beta . ⁇ lb ⁇ lb ⁇ gt ; ⁇ me : `` ⁇ dev , can you rename that branch because we 're going to use just two . I do n't want to keep up with 80 quintilian branch . '' ⁇ lb ⁇ gt ; ⁇ dev : `` sure , one second . '' ⁇ lb ⁇ lb ⁇ Five minute later ... ⁇ lb ⁇ lb ⁇ gt ; ⁇ dev : `` [ CurseWords ] I want beta1 to die ! '' ⁇ lb ⁇ gt ; ⁇ me : `` What happened ? '' ⁇ lb ⁇ lb ⁇ Lots of removed dialog where ⁇ dev explains what he did ... ⁇ lb ⁇ lb ⁇ gt ; ⁇ me : `` Did you try ⁇ Popular ⁇ ersionControl with -u ? '' ⁇ lb ⁇ gt ; ⁇ dev : `` [ Cursing ] That would be why ! '' ⁇ lb ⁇ lb ⁇ In short . Always check your command line switch ... They are important ! ⁇ lb ⁇\n",
      "1    So what wa Matt `` addicted '' to ? . Did he ever say what his addiction wa or is he still chugging beer while talking about how sober he is ? ⁇ lb ⁇ lb ⁇ Edited to add : As an addict myself , anyone I know whose been an addict doe n't drink and in the group I go to ( similar to NA and AA ) drinking is considered a slip-up . Has he said what he 's addicted to or is he still talking out of his as , winging it a he go ?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      "2    No Club Colors . Funny story . I went to college in Las ⁇ egas . This wa before I knew anything about motorcycling whatsoever . Me and some college buddy would always go out on the strip to the dance club . We always ended up at a bar called Hogs ⁇ amp ; Heifers . It 's worth noting the female working there can outdrink ANYONE . Anyway , there wa a sign on the front door that read 'No Club Colors ' . So we lose our tie and blazer before heading there . Also we assumed bright color like red , yellow , green etc were not allowed . So we would always bring an xtra t-shirt and pair of jean . This went on for year ! Looking back now on how naive we were , it 's just hilarious . I wa never able to walk out of that bar .... had to crawl out ! So much booze . ⁇ lb ⁇ lb ⁇ Cheers . Ride safe , boy !                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "3    Not door bell , but floodlight mount height . . I know this is a sub for the 'Ring Doorbell ' but ha anyone used the Floodlight ? I already have the wire and existing bracket for the floodlight on the back of my house , but the problem is that it 's about 12 foot above ground level ( 10 ft above the deck , 2 ft drop from the deck down to the grass ) ⁇ lb ⁇ lb ⁇ Is that too high to mount ? The website say ⁇ ft is ideal . Anyone had any problem mounting it higher than that ?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-30f7f08f06f3>:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(dataset.loc[:3, \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def encode(sentence, num_tokens : int, encoder_dict : dict, embeddings_dict : dict) -> np.ndarray:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    for word in tokenized:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower not in encoder_dict and word_lower in embeddings_dict:\n",
    "            encoder_dict[word_lower] = len(encoder_dict)\n",
    "    array = np.ones((num_tokens))\n",
    "    for i, word in enumerate(tokenized):\n",
    "        if i >= num_tokens:\n",
    "            break\n",
    "        array[i] = encoder_dict.get(word.lower(), 0)\n",
    "    return array\n",
    "\n",
    "def decode(encoded_sentence, decoder_dict):\n",
    "    return \" \".join([decoder_dict[idx] for idx in encoded_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\embeddings\\\\glove_reddit_dict.pkl\", \"rb\") as file:\n",
    "    embeddings_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1013000/1013000 [14:22<00:00, 1173.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# num_tokens = 256\n",
    "# encoder_dict = {}\n",
    "# decoder_dict = {}\n",
    "# encoder_dict[\"⁇\"] = 0\n",
    "# encoder_dict[\"<pad>\"] = 1\n",
    "# X_encoded = np.array(dataset[\"text\"].progress_apply(functools.partial(encode, num_tokens=num_tokens, encoder_dict=encoder_dict, embeddings_dict=embeddings_dict)))\n",
    "# # for i in tqdm(range(dataset.shape[0])):\n",
    "# #     dataset[\"text\"][i] = encode(dataset[\"text\"][i], num_tokens, encoder_dict)\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(dataset[\"subreddit\"])\n",
    "# y_dict = le.get_params()\n",
    "# X_encoded = np.stack(X_encoded).astype(np.int64)\n",
    "\n",
    "# with open(\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\dataset_normalized\\\\X_y_encoderDict_le_preprocessed.pkl\", \"wb\") as file:\n",
    "#     pickle.dump((X_encoded,y,encoder_dict,le), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202, 184, 226,  10, 227, 228, 229, 230,   7,   7,  33, 133,  24,\n",
       "        25,  58, 231, 208,  12,   0, 232, 187, 227, 233, 132, 234,  12,\n",
       "       228,  91,  33, 235, 236,  12, 237, 138, 238, 239, 208,  12, 228,\n",
       "       166,  12, 211,  93,  40, 240,  10, 227,  12, 241,  25,  60, 150,\n",
       "        31, 124, 242, 243, 244, 245, 246,   0, 247, 248, 244,  12, 249,\n",
       "        10, 250, 248, 251, 252,  12, 249, 253,  46,  12, 254,   0,   0,\n",
       "        11,   0,  11,   0,  25,  60, 255, 256,  46, 229,  91,  12, 257,\n",
       "       115,   0, 248,  25, 258,   7, 132, 218, 259, 241, 260, 150, 261,\n",
       "       262,  60,  91,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoded[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_sparse_to_pytorch(matrix):\n",
    "    values = matrix.data\n",
    "    indices = np.vstack((matrix.tocoo().row, matrix.tocoo().col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = matrix.shape\n",
    "\n",
    "    return torch.sparse_coo_tensor(i, v, torch.Size(shape), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([162080, 256]),\n",
       " torch.Size([40520, 256]),\n",
       " torch.Size([810400, 256]),\n",
       " torch.Size([162080]),\n",
       " torch.Size([40520]),\n",
       " torch.Size([810400]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "random_seed = 42\n",
    "with open(\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\dataset_normalized\\\\X_y_encoderDict_le_preprocessed.pkl\", \"rb\") as file:\n",
    "    X, y, encoder_dict, le = pickle.load(file)\n",
    "# ohe = OneHotEncoder()\n",
    "# y = ohe.fit_transform(y.reshape(-1,1)).astype(np.int64)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_seed)\n",
    "X_train, X_val, X_test = tuple([torch.tensor(array.astype(np.int64)) for array in [X_train, X_val, X_test]])\n",
    "# y_train, y_val, y_test = tuple([scipy_sparse_to_pytorch(array) for array in [y_train, y_val, y_test]])\n",
    "y_train, y_val, y_test = tuple([torch.tensor(array.astype(np.int64)) for array in [y_train, y_val, y_test]])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "device = torch.device(\"cuda\")\n",
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "\n",
    "\n",
    "def get_dataloader(X_train : torch.tensor, X_val : torch.tensor, y_train : torch.tensor, y_val : torch.tensor, batch_size : int=20000, num_workers : int=8):\n",
    "    train_dataloader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, num_workers=num_workers)\n",
    "    val_dataloader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, num_workers=num_workers)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "def load_vectors_to_np_arrays(encoder_dict : dict) -> np.ndarray:\n",
    "    with open(\"tmp\\\\dataset_dumps\\\\preprocess_dumps\\\\embeddings\\\\glove_reddit_dict.pkl\", \"rb\") as file:\n",
    "        embeddings_dict = pickle.load( file)\n",
    "    embeddings_dim = embeddings_dict[\"hello\"].shape[0]\n",
    "    embeddings = np.zeros((len(encoder_dict), embeddings_dim))\n",
    "    for key, idx in encoder_dict.items():\n",
    "        embeddings[idx] = embeddings_dict.get(key, np.zeros((embeddings_dim))).astype(np.float32)\n",
    "    return torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_dataloader(X_train, X_val, y_train, y_val, batch_size=1000, num_workers=8)\n",
    "embeddings = load_vectors_to_np_arrays(encoder_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(random_state : int):\n",
    "    torch.manual_seed(random_state)\n",
    "    torch.cuda.manual_seed(random_state)\n",
    "    torch.cuda.manual_seed_all(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings : torch.tensor=None, freeze_embeddings=False, emb_n_dims=300, vocab_size : int=None, token_seq_size=256, n_classes=1013):\n",
    "        super(CNN, self).__init__()\n",
    "        self.token_seq_size = token_seq_size\n",
    "        self.conv_kernel_size = 10\n",
    "        self.conv_out = 100\n",
    "        self.unfreeze_epoch = 30\n",
    "        self.n_classes = n_classes\n",
    "        self.freeze_embeddings = freeze_embeddings\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.vocab_size, self.emb_n_dims = pretrained_embeddings.shape\n",
    "            self.embeddings_layer = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "        else:\n",
    "            self.vocab_size, self.emb_n_dims = vocab_size, emb_n_dims\n",
    "            self.embeddings_layer = nn.Embedding(num_embeddings=vocab_size, \\\n",
    "                                                                  embedding_dim=self.emb_n_dims,padding_idx=0, max_norm=5.0)\n",
    "            \n",
    "        self.conv = nn.Conv1d(self.emb_n_dims,self.conv_out,kernel_size=self.conv_kernel_size,padding=4)\n",
    "        self.conv_kernel_sizes= [5, 15, 30]\n",
    "        self.convolutions = nn.ModuleList([nn.Conv1d(self.emb_n_dims,self.conv_out,kernel_size=kernel_size,padding=4) for kernel_size in self.conv_kernel_sizes])\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.conv_out * len(self.convolutions), self.conv_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.conv_out, self.n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embeddings_layer(input_ids).float()\n",
    "#         print(\"embeddings\", x.shape, sep=' ')\n",
    "#         print(x)\n",
    "#         return None\n",
    "        x = x.permute(0, 2, 1)\n",
    "#         print(\"permute\", x.shape, sep=' ')\n",
    "#         x = self.conv(x)\n",
    "        x = [conv1d(x) for conv1d in self.convolutions]\n",
    "#         print(\"conv\", x.shape, sep=' ')\n",
    "        x = [F.relu(conv_res) for conv_res in x]\n",
    "#         print(\"relu\", x.shape, sep=' ')\n",
    "        x = [F.max_pool1d(conv_res, conv_res.shape[2]) for conv_res in x]\n",
    "#         print(\"pooling\", (len(x), len(x[0])), sep=' ')\n",
    "        x = torch.cat([conv_res.squeeze(dim=2) for conv_res in x], dim=1)\n",
    "        x = self.linear(x)\n",
    "#         print(\"linear\", x.shape, sep=' ')\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, optimizer, scheduler, criterion, dataloader, n_iter : int):\n",
    "        self = self.cuda()\n",
    "        self.train()\n",
    "        for epoch in range(n_iter):\n",
    "            if not(self.freeze_embeddings) and epoch == self.unfreeze_epoch:\n",
    "                self.embeddings_layer.weight.requires_grad = True\n",
    "                print(\"EMBEDDINGS UNLEASHED ! ! !\")\n",
    "            for X, y in dataloader:\n",
    "                X = X.view(-1, self.token_seq_size)\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output = self(X)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                del X\n",
    "                del y\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"ep{epoch}: {loss}\")\n",
    "                path = f\"tmp\\\\model_dumps\\\\cnn_text\\\\model_{self.conv_out}conv_{str(self.conv_kernel_sizes).replace(', ', '_')}kernelsize_{n_iter}ep_freezeemb_{self.freeze_embeddings}.pt\"\n",
    "                print(optimizer.param_groups[0][\"lr\"])\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    }, path\n",
    "                )\n",
    "            scheduler.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 5e-01\n",
    "n_iter = 100\n",
    "model = CNN(embeddings, freeze_embeddings=True, emb_n_dims=300, vocab_size=len(encoder_dict))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=1e-05)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep0: 6.916538238525391\n",
      "0.5\n",
      "ep10: 2.6911909580230713\n",
      "0.40853640344377334\n",
      "ep20: 0.8724244236946106\n",
      "0.3338039858775471\n",
      "ep30: 0.48165076971054077\n",
      "0.27274215969121846\n",
      "ep40: 0.30965179204940796\n",
      "0.22285020197547536\n",
      "ep50: 0.22045207023620605\n",
      "0.18208484004355835\n",
      "ep60: 0.1865234673023224\n",
      "0.1487765713460602\n",
      "ep70: 0.08325999975204468\n",
      "0.12156129074883078\n",
      "ep80: 0.18253296613693237\n",
      "0.09932442504102033\n",
      "ep90: 0.09914611279964447\n",
      "0.08115528676075824\n"
     ]
    }
   ],
   "source": [
    "model.fit(optimizer, scheduler, criterion, train_dataloader, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6039,\n",
       " 'f1_score': 0.5937924449485792,\n",
       " 'precision_score': 0.615643462199326,\n",
       " 'recall_score': 0.6031703427817491}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def test(model, X, y): \n",
    "    X = X.cpu()\n",
    "    model = model.cpu()\n",
    "    model = model.eval()\n",
    "    y_pred = torch.argmax(model(X), axis=1)\n",
    "    metrics = {\"accuracy\": accuracy_score(y, y_pred),\n",
    "               \"f1_score\": f1_score(y, y_pred, average=\"macro\"),\n",
    "               \"precision_score\": precision_score(y, y_pred, average=\"macro\"),\n",
    "               \"recall_score\": recall_score(y, y_pred, average=\"macro\")}\n",
    "#     print(y_pred, y_test)\n",
    "    return metrics\n",
    "\n",
    "test(model, X_test[:10000], y_test[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(train_dataloader)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11010 cuda compiled version\n"
     ]
    }
   ],
   "source": [
    "print(torch._C._cuda_getCompiledVersion(), 'cuda compiled version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPAPEC\\anaconda3\\envs\\coursework\\lib\\site-packages\\torch\\nn\\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep0: 6.914767265319824\n",
      "0.5\n",
      "ep10: 3.0262460708618164\n",
      "0.40853640344377334\n",
      "ep20: 0.9993476867675781\n",
      "0.3338039858775471\n",
      "EMBEDDINGS UNLEASHED ! ! !\n",
      "ep30: 0.37133246660232544\n",
      "0.27274215969121846\n",
      "ep40: 0.16381075978279114\n",
      "0.22285020197547536\n",
      "ep50: 0.1938459575176239\n",
      "0.18208484004355835\n",
      "ep60: 0.18892142176628113\n",
      "0.1487765713460602\n",
      "ep70: 0.06988176703453064\n",
      "0.12156129074883078\n",
      "ep80: 0.06524232029914856\n",
      "0.09932442504102033\n",
      "ep90: 0.09676981717348099\n",
      "0.08115528676075824\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "set_seed(42)\n",
    "\n",
    "learning_rate = 5e-01\n",
    "n_iter = 100\n",
    "model = CNN(embeddings, freeze_embeddings=False, emb_n_dims=300, vocab_size=len(encoder_dict))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=1e-05)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "model.fit(optimizer, scheduler, criterion, train_dataloader, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6311,\n",
       " 'f1_score': 0.6190168009433548,\n",
       " 'precision_score': 0.6397481581613659,\n",
       " 'recall_score': 0.628543757167584}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def test(model, X, y): \n",
    "    X = X.cpu()\n",
    "    model = model.cpu()\n",
    "    model = model.eval()\n",
    "    y_pred = torch.argmax(model(X), axis=1)\n",
    "    metrics = {\"accuracy\": accuracy_score(y, y_pred),\n",
    "               \"f1_score\": f1_score(y, y_pred, average=\"macro\"),\n",
    "               \"precision_score\": precision_score(y, y_pred, average=\"macro\"),\n",
    "               \"recall_score\": recall_score(y, y_pred, average=\"macro\")}\n",
    "#     print(y_pred, y_test)\n",
    "    return metrics\n",
    "\n",
    "test(model, X_test[:10000], y_test[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
